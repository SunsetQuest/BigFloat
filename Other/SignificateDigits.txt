===================================================‎ 1/14/2024 ===================================================
Precision:

One of this project's goals was to maintain accuracy. Early in the project, it dropped off the least significant bits. However, as the project evolved, rounding the least significant bits became more crucial. 
In the perfect world, BigFloat would always be accurate to the bit. We can get close to this, but there are limits. We use some different techniques to help with this. First, we can have some trailing extra hidden bits considered out-of-precision at the end. For BigFloat, we call these hidden bits. By default, BigFloat has 32 out-of-precision hidden bits. When a user loads an integer 5, they would get 101|000...000, which is 35 bits in size.  
With these numbers, we can do math operations. When we have a number with, say, five in-precision bits and perform a math operation on another number, say multiplying by 3, we generally expect a result with the same number of significant digits as the smaller number. However, with the math operation, we often get some extra digits(or bits) that we must remove, and with these, we should round to the nearest as were we tough.

How is rounding done:

Rounding for BigFloat is simple. After a math operation, we may have some least-significate digits to remove to preserve precision. If the most significant bit we remove is set, we increment the least significant digit of the number.

Example of ~6 x ~4:

   101|11001011101101001000101100110100
 x 100|01011001101001011100101110110101
-----------------------------------------------------
110|11001100100110110010011001111111[101100...]
110|11001100100110110010011001111111 (dropped)
110|11001100100110110010011010000000 (rounded)
* "|" is the separator for the in-precision and out-of-precision hidden bits.
** "[]" bits past the hidden bits - to be rounded
Even though these bits were in the hidden area and are considered out-of-precision, Rounding helps with the loss of precision with successive math operations operating on it. The precision slowly decreases with chopping off the bits (i.e., rounding down). However, if Rounding is done correctly for some math functions, the Rounding up and down of the least significant digit will cancel each other out over time. This is equivalent to counting the number of heads when flipping a coin several times.

Example with dropping bits:

A trillion serial math operations with an average of half rounding down incorrectly (when we should be rounding up) would result in a value roughly half a billion too low. So, the bottom 40 bits are incorrect on average, and this would be above our 32 hidden bits, making it incorrect.

Example with rounding bits:

With Rounding, one million serial math operations would produce 25% that round up incorrectly, 25% that round down incorrectly, and 50% that is correct. That standard deviation, or average incorrect value, would be Sqrt(trillion/2·1/2·1/2)1/2 = 176776, so only the bottom 18 bits are incorrect on average, still well within our 32 hidden bits, making it incorrect. 
So, how many math operations do we need before we use up all the 32 hidden out-of-precision bits? Solving for x in Sqrt(X/2·1/2·1/2)1/2=2^32 = 6 x 10^20. So, after 600000000000000000000 math operations, we would have used up our hidden bits and would be off by one on average. If we enlarged our hidden bits from 32 to 64, this number would grow to 1x10^40.

Rounding on IEEE floating point math is very similar. Single and Double operations even do Banker's Rounding, so if the numbers we remove are in a one followed by zeros, it will only round up half the time. Banker's Rounding is more critical for the IEEE floating points as it only keeps a few extra bits; however, BigFloat will keep more, so Banker's Rounding is less critical. BigFloat does have some other advantages over IEEE Float. 
 - Since BigFloat is a software library, it is more adjustable. 
 - additional extra hidden precision bits, and finally, 
 - BigFloat has flexible precision (vs. Single/Double with fixed 26-bit/53-bit sizes). 
One massive advantage of Single/Double is that it is fast and efficient since it is hardware-based. It just does not handle large numbers and flexible precision.
BigFloat, however, does use double's hardware in several places. For Example, it will leverage Double's hardware floating point function to get the first 53 bits of a square root.

===================================================‎ Friday, ‎November ‎10, ‎2023 ===================================================

Going beyond significant digits (in binary/decimal)

-> I recently built a floating point class for large numbers, so I temporarily know this very well. =)

 - Pre notes:  4
     - No integers are used here. Integers have infinite precision. (not mentioned here)
	     - Example: A block weighs 123.4 kg. How much would 10 blocks weigh? 10.000000...  x 123.4 = 123.4

Decimal is not the best number base for precision... because of forced precision.
 - Binary has 3.3x better precision and, in some cases, perfect precision.
 - Binary can also be multiplied by 2,4,8 etc by just shifting

Learned that..
 - The significant digits taught in schools are roughly correct.
 - Multiply two numbers, and the result is the smaller of the two. This is close, but it's more complicated than that.
 - There is such a thing as partial precision.
     - 9999.9 is 5 digits but just adding 0.1 leads to a full 6 digits (10000.0)?  
	 - This is true if we are roughly approximating and rounding to the nearest whole digit.
	 - In reality, however, it would be... 
	     - dec: "1 + Log10(number)"   9999.9 -> 4.999995657  ;   10000.0 -> 5.0000
		 - bin: "1 +  Log2(number)"   1111.1 -> 3.954        ;   10000.0 -> 5.0000
		 
 - When adding numbers (greater than 0), you always gain precision. 
     2.1
    +1.5
	====
     3.6  (aka a range of 3.55 to 3.65)
     	 
	 - It looks like we gain 0 to 1 bits for binary and decimal.  
	     - decimal: 100+100=200    99+99=198
		 - binary:  100+100=1000   11+11=100   (BTW, more beautiful)
		 
	 - But do we get more precision??? Let's try the extremes to see what the highest and lowest would look like...
	         2.05    2.15
	        +1.45   +1.55
	        =====   =====
	         3.40 to 3.70  <-- range using extreme values
             3.55 to 3.65  <-- range from traditional precision (above)
	 
	 - The earlier classical one is an easy trap. They are for integers or decimals with perfect precision.
	     Example:
	        2.1000000...
           +1.5000000...
            ============
            3.6000000...

	 
 - When subtracting numbers (greater than 0), you always reduce the precision. 
     2.1        1000
    -1.5        -111	
	====        ====
     0.6           1
     	 
	 - result size will range from 1 to just under the length of the longest (we can lose almost all digits)
	     - decimal: 100+100=200    99+99=198
		 - binary:  100+100=1000   11+11=100   (BTW, more beautiful)

	 - But again, this is not correct. The extreme values look like...
	         2.05    2.15
	        -1.55   -1.45
	        =====   =====
	         0.5  to 0.7   <-- range using extreme values
             0.55 to 3.65  <-- range from traditional precision (above)
	 

	 
 - When multiplying numbers..
     - The number of significant digits is more complicated. 
	 - In grade school, they taught us the result should be smaller than the two. This is roughly correct but usually overestimates precision.
	 - The best way to figure out precision for an operation is to take the extreme cases. In this case, the smallest and largest.
	 - Say we multiply 12 and 17. For the smallest extreme, the 12 could be 11.5, and the 17 could be 16.5. Or the largest extreme would be 12.5 x 17.5.
	 
	  Simple Mult       Low Extream      High Extreme
          12               11.5              12.5
        x 17             x 16.5            x 17.5
       	====              =====             =====
         204              189.75            218.75   
       
	   So, we took two numbers of 2 digits of precision and ended up with no digits of precision. You can see here that the grade school "20" is very rough and estimates the precision. 

	   In a perfect world, we would keep track of the low and high values. (or the low and the distance to the larger value)
	   This is a critical point.  
	  
	   More Accurately, it would be...
	        204.25 +/- 14.5    (218.75 is not inclusive here)
		 or 189.75 to 218.75   (218.75 is not inclusive here)
		 or 189.75 + (0 to 29)  <-- I like this type as the "29" is just 12+17.
	  	 
		 
	 - result size will range from 1 to just under the length of the longest (we can lose almost all digits)
	     - decimal: 100+100=200    99+99=198
		 - binary:  100+100=1000   11+11=100   (note: more beautiful)
		 
	 - so, putting this into a formula, the range can be found by:
	     Low end:  A.low x B.low        (".low" is the lowest value it can represent. example: 3.4 would be 3.35)
		 High end: Low + A + B


 - When Dividing numbers...
     - Divide is just repeated subtraction and we can use high and low ranges also.
	 
	  Simple Mult       Low Extream      High Extreme
          12               11.5              12.5
        / 17             / 17.5            / 16.5
       	====              =====             =====
        0.705882352     0.6571428         0.7575757
		 

				   
 - Other stuff
     - How to perform operations with number ranges.
	     - For Multiply, for example, the lows  would be multiplied by lows
	     - For Multiply, for example, the highs would be multiplied by highs
 
 
     - Should we not round at .5??
	   I think 1.1 would round to 1, and 1.4 would round to 2.
	   so, when we say 12 x 17, we would be saying the ranges high and low ranges would be:
	       0: -.5 to .5
		   1:  .5 to 1.41421356 
		   2:  
		   3: 


    |-------|-------|-------|-------|-------|-------|-------|
   -2      -1       0       1       2       3       4       5
		              -1    0  .58  1 1.32 1.58     2     2.32
					     1+0/1   1+1/2   1+1/3   1+1/4
						   

		   12 3.585
		   17 4.0875

	 